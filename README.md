# üß† AML‚ÄìLLM Hybrid Simulator

Train a compact Llama-3 model to emulate a Drools-based retail banking AML engine. This repo takes symbolic rules, generates synthetic labeled data, fine-tunes a QLoRA adapter, deploys it with vLLM, and benchmarks the model against the original rule engine.

---

## üöÄ Project Goals
1. **Codify business rules** in Drools (`rule-engine/`) and treat them as the ground truth.
2. **Mass-produce training data** (100k+ JSON cases) via `data-gen/`.
3. **Fine-tune a small Llama model** using lightweight Hugging Face/PEFT tooling (`training/`).
4. **Serve the adapter** with vLLM behind an OpenAI-compatible API (`serve/`).
5. **Benchmark & compare** the LLM vs the rule engine (accuracy, latency, throughput) using scripts in `benchmark/`.

---

## üìÅ Repository Layout
```
aml-llm/
‚îú‚îÄ‚îÄ benchmark/                # Benchmarking, comparison, Drools vs LLM tooling
‚îú‚îÄ‚îÄ data-gen/                 # Synthetic data generator and dataset utilities
‚îú‚îÄ‚îÄ rule-engine/              # Drools runner, rules, and HTTP service wrapper
‚îú‚îÄ‚îÄ serve/                    # vLLM runtime scripts + deployment notes
‚îú‚îÄ‚îÄ training/                 # Fine-tuning scripts (local + Vertex AI helpers)
‚îî‚îÄ‚îÄ README.md                 # This file
```

---

## üîß Prerequisites
* Java 17+, Maven 3.9+
* Python 3.10+
* CUDA-capable GPU (L4/A10/A100/H100) for training + serving
* Hugging Face access to [`meta-llama/Llama-3.2-3B-Instruct`](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
* (Optional) Google Cloud CLI for Vertex AI workflows

Environment variables commonly used:
```bash
export HF_TOKEN=hf_xxx                        # Hugging Face access token
export BASE_MODEL_ID=meta-llama/Llama-3.2-3B-Instruct
```

---

## üìö Quickstart
1. **Build the Drools runner**
   ```bash
   cd rule-engine/drool-runner
   mvn -q -DskipTests package            # produces target/drools-runner-1.0.0-shaded.jar
   ```
2. **Generate a dataset**
   ```bash
   cd data-gen
   python make_tx_aml_dataset.py 50000 dataset/tx_aml_dataset.jsonl
   python split_dataset.py dataset/tx_aml_dataset.jsonl
   ```
3. **Fine-tune locally (QLoRA)**
   ```bash
   pip install -r training/llm_training/local-training-scripts/requirements.txt
   python -m training.llm_training.common.train \
     --method qlora \
     --model_id $BASE_MODEL_ID \
     --dataset_file data-gen/dataset/tx_aml_dataset.jsonl \
     --output_dir training/llm_training/local-training-scripts/my-finetuned-model
   ```
4. **Serve with vLLM**
   ```bash
  pip install -r serve/serve-llm/requirements.txt
   export HF_TOKEN=hf_xxx
  export ADAPTER_DIR=/opt/aml-llm/serve/serve-llm/artifacts   # downloaded adapter path
  ./serve/serve-llm/run_vllm_server.sh                        # exposes http://0.0.0.0:8000/v1
   ```
5. **Benchmark the endpoint**
   ```bash
   pip install -r benchmark/requirements.txt
   BASE_URL=http://<llm-host>:8000 MODEL=aml-qlora \
     benchmark/run_llm_benchmark.sh \
       data-gen/dataset/tx_aml_dataset.jsonl \
       reports/llm.txt \
       --limit 2000
   ```

---

## üß© End-to-End Workflow

### 1. Rule Engine (`rule-engine/`)
* **`drool-runner/`** ‚Äì Maven project that wraps Drools rules and accepts JSON I/O.
* **`rules/tx_aml.drl`** ‚Äì Canonical AML rules.
* **`drools_service.py`** ‚Äì FastAPI wrapper to expose the runner at `/v1/drools/score` for benchmarking.

> Build the runner _once_ before generating data or benchmarking:
> ```bash
> cd rule-engine/drool-runner && mvn -q -DskipTests package
> ```

### 2. Data Generation (`data-gen/`)
* **`make_tx_aml_dataset.py`** ‚Äì Creates balanced scenarios (CLEAR/REVIEW/SAR/BLOCK), shells out to the Drools jar for ground-truth labels, and writes JSONL plus audit files.
* **`split_dataset.py`** ‚Äì 80/10/10 train/val/test split.
* **`quality_check.py`** ‚Äì Coverage diagnostics (reasons, fired rules, decision balance).

Key knobs inside `make_tx_aml_dataset.py`:
* Scenario priors (`SCENARIO_PROBS`) and class ratios (`TARGET_RATIOS`).
* KYC profiles, transaction generators, and reason deduping.

### 3. Training (`training/`)
* **`common/train.py`** ‚Äì Unified CLI with `--method` = `qlora`, `lora`, or `full`.
  * **`llm_training/local-training-scripts/`** ‚Äì Requirements + examples for single-GPU runs.
  * **`transformer_training/`** ‚Äì From-scratch structured transformer, multi-head classifier.
* **`cloud-training-script/`** ‚Äì Vertex AI helpers (`submit_vertex_job.sh`, `submit_transformer_job.sh`) that build trainer images, upload data to GCS, and launch A100/H100 jobs.
* **`training-api/`** ‚Äì Express REST wrapper that lets DeepFinery Studio kick off and monitor training jobs via HTTP.

Typical local run:
```bash
python -m training.llm_training.common.train \
  --method qlora \
  --model_id $BASE_MODEL_ID \
  --dataset_file data-gen/dataset/tx_aml_dataset.jsonl \
  --output_dir my-finetuned-model \
  --num_train_epochs 2 \
  --per_device_train_batch_size 4

### Train the structured transformer (multi-head)
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r training/transformer_training/requirements.txt

python -m training.transformer_training.train \
  --dataset_file data-gen/dataset/tx_aml_dataset.jsonl \
  --output_dir training/transformer_training/artifacts \
  --epochs 5 \
  --batch_size 64

# Docker smoke test (one epoch, local GPU)
cd training/cloud-training-script
./run_transformer_local.sh

# Reuse an existing local image without rebuilding
./run_transformer_local_fast.sh

# Vertex AI one-click transformer training
cd training/cloud-training-script
./submit_transformer_job.sh

# Serve the trained transformer locally
python serve/serve-transformer/serve_transformer.py \
  --checkpoint training/transformer_training/artifacts/transformer_multihead.pt \
  --host 0.0.0.0 \
  --port 9000
```

Vertex workflow highlights:
1. `gcloud auth login` + `gcloud config set project ...`
2. `cd training/cloud-training-script && ./submit_vertex_job.sh` (LLM LoRA pipeline) or `./submit_transformer_job.sh` (structured transformer pipeline).
3. Monitor with `gcloud ai custom-jobs stream-logs JOB_ID --region=$REGION`

### 4. Serving (`serve/`)
* **`download_artifacts.sh`** ‚Äì Sync PEFT adapters from GCS to the VM.
* **`run_vllm_server.sh`** ‚Äì Launches `vllm.entrypoints.openai.api_server` with bitsandbytes, eager mode, and the LoRA adapter registered as `aml-qlora`.
  * Environment knobs: `ADAPTER_DIR`, `PORT`, `HOST`, `MAX_LORA_RANK`, `RUN_IN_BACKGROUND`, etc.
  * Endpoint: `http://HOST:PORT/v1/chat/completions` (OpenAI-compatible). Call with `model: "aml-qlora"` to apply the adapter.

Health check example:
```bash
curl -s http://<host>:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "aml-qlora",
        "temperature": 0.0,
        "max_tokens": 128,
        "messages": [
          {"role": "system", "content": "You are an AML rule engine simulator. Return JSON only."},
          {"role": "user", "content": "Facts: {\"amount\": 54000, \"channel\": \"WIRE\", \"hour\": 2, \"country\": \"DE\"}"}
        ]
      }'
```

### 5. Benchmarking & Comparison (`benchmark/`)
* **`benchmark_endpoint.py`** ‚Äì Core evaluator for the LLM endpoint (accuracy, reasons F1, latency, throughput). Accepts `--summary-json` for downstream tooling.
* **`run_llm_benchmark.sh`** ‚Äì Convenience wrapper that writes both human-readable (`.txt`) and machine-readable (`.json`) summaries.
* **`benchmark_drools.py`** ‚Äì Replays the dataset against the Drools HTTP service to capture baseline metrics.
* **`drools_service.py`** (in `rule-engine/`) ‚Äì FastAPI proxy so the rule engine can be benchmarked like any other HTTP service.
* **`compare_benchmarks.py`** ‚Äì Consumes the JSON summaries and prints a table (decision accuracy, avg latency, throughput, exact match).
* **`run_transformer_vs_drools.sh`** ‚Äì Generates fresh cases, times Drools and the structured transformer side-by-side, and writes accuracy/latency metrics.

Workflow:
```bash
# 1) Run LLM benchmark
BASE_URL=http://<llm-host>:8000 MODEL=aml-qlora \
  benchmark/run_llm_benchmark.sh data-gen/dataset/tx_aml_dataset.jsonl reports/llm.txt --limit 2000

# 2) Serve Drools and benchmark it
python rule-engine/drools_service.py --port 9000
python benchmark/benchmark_drools.py \
  --dataset data-gen/dataset/tx_aml_dataset.jsonl \
  --base-url http://localhost:9000 \
  --summary-json reports/drools.json

# 3) Compare
python benchmark/compare_benchmarks.py reports/llm.json reports/drools.json

# 4) Transformer vs Drools head-to-head (new cases)
cd benchmark
./run_transformer_vs_drools.sh \
  SAMPLES=1000 \
  CHECKPOINT=../training/transformer_training/local_artifacts/transformer_multihead.pt
```

---

## üó∫Ô∏è Architecture Summary
```text
rule-engine/rules (Drools)
   ‚Üì  mvn package
rule-engine/drool-runner (shaded jar)
   ‚Üì  subprocess
data-gen/make_tx_aml_dataset.py (JSONL + audit)
   ‚Üì
training/llm_training/common/train.py (LoRA/QLoRA/full)
   ‚Üì
serve/serve-llm/run_vllm_server.sh (vLLM OpenAI API, LoRA adapter)
   ‚Üì
benchmark/{benchmark_endpoint, benchmark_drools}.py + compare
```

---

## üß≠ Roadmap & Ideas
* Expand rule coverage (odd channels, FX, sanctions lists) and regenerate data.
* Integrate hard-example mining (misclassified cases) back into Drools to tighten guardrails.
* Wire benchmarks into CI/CD or scheduled cron to catch regressions.
* Explore larger adapters (rank > 32) or full SFT once hardware permits.

---

## ü§ù Contributions
1. Fork the repo (`https://github.com/deepfinery/rule-finery`), create a topic branch, and submit a PR.
2. Please keep instructions up to date when moving files or changing scripts (e.g., new rule-engine paths).
3. Open issues for bugs, missing docs, or ideas‚Äîwe‚Äôre building the AML simulator in the open.

Happy hacking! üõ°Ô∏è
